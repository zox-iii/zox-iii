{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # SVM:使用软间隔最大化，SMO优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *  \n",
    "import time  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calulate kernel value  \n",
    "def calcKernelValue(matrix_x, sample_x, kernelOption):  \n",
    "    kernelType = kernelOption[0]\n",
    "    numSamples = matrix_x.shape[0]  \n",
    "    kernelValue = mat(zeros((numSamples, 1)))  # n*1\n",
    "     \n",
    "    if kernelType == 'linear':   # 线性函数 \n",
    "        kernelValue = matrix_x * sample_x.T  \n",
    "    elif kernelType == 'rbf':    # 径像核函数/高斯核 \n",
    "        sigma = kernelOption[1]  \n",
    "        if sigma == 0:  \n",
    "            sigma = 1.0  \n",
    "        for i in range(0,numSamples):  # \n",
    "            diff = matrix_x[i, :] - sample_x  \n",
    "            kernelValue[i] = exp(diff * diff.T / (-2.0 * sigma**2))  \n",
    "    else:  \n",
    "        raise NameError('Not support kernel type! You can use linear or rbf!')  \n",
    "    return kernelValue   # return a matrix(n*1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kernel matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate kernel matrix given train set and kernel type  \n",
    "def calcKernelMatrix(train_x, kernelOption):  \n",
    "    numSamples = train_x.shape[0]  \n",
    "    kernelMatrix = mat(zeros((numSamples, numSamples)))  # n*n\n",
    "    for i in range(0,numSamples):  \n",
    "        kernelMatrix[:, i] = calcKernelValue(train_x, train_x[i, :], kernelOption)  \n",
    "    return kernelMatrix  # return a mztrix(n*n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a struct just for storing variables and data  \n",
    "class SVMStruct:  \n",
    "    def __init__(self, dataSet, labels, C, toler, kernelOption):  \n",
    "        self.train_x = dataSet # each row stands for a sample  \n",
    "        self.train_y = labels  # corresponding label  \n",
    "        self.C = C             # slack variable   \n",
    "        self.toler = toler     # termination condition for iteration\n",
    "        self.numSamples = dataSet.shape[0] # number of samples  \n",
    "        self.alphas = mat(zeros((self.numSamples, 1))) # Lagrange factors for all samples\n",
    "        self.b = 0  \n",
    "        self.errorCache = mat(zeros((self.numSamples, 2)))\n",
    "        self.kernelOpt = kernelOption  \n",
    "        self.kernelMat = calcKernelMatrix(self.train_x, self.kernelOpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the error for alpha k  \n",
    "def calcError(svm, alpha_k):  \n",
    "    output_k = float(multiply(svm.alphas, svm.train_y).T * svm.kernelMat[:, alpha_k] + svm.b) \n",
    "    error_k = output_k - float(svm.train_y[alpha_k])\n",
    "    return error_k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the error cache for alpha k after optimize alpha k  \n",
    "def updateError(svm, alpha_k):  \n",
    "    error = calcError(svm, alpha_k)  \n",
    "    svm.errorCache[alpha_k] = [1, error] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select alpha j which has the biggest step\n",
    "def selectAlpha_j(svm, alpha_i, error_i):  \n",
    "    svm.errorCache[alpha_i] = [1, error_i] # mark as valid(has been optimized)  \n",
    "    candidateAlphaList = nonzero(svm.errorCache[:, 0].A)[0] # mat.A return array;\n",
    "    maxStep = 0; alpha_j = 0; error_j = 0  \n",
    "  \n",
    "    # find the alpha with max iterative step  \n",
    "    if len(candidateAlphaList) > 1:  \n",
    "        for alpha_k in candidateAlphaList:  \n",
    "            if alpha_k == alpha_i:   \n",
    "                continue  \n",
    "            error_k = calcError(svm, alpha_k)  \n",
    "            if abs(error_k - error_i) > maxStep:  \n",
    "                maxStep = abs(error_k - error_i)  \n",
    "                alpha_j = alpha_k  \n",
    "                error_j = error_k  \n",
    "    # if came in this loop first time, we select alpha j randomly  \n",
    "    else:             \n",
    "        alpha_j = alpha_i  \n",
    "        while alpha_j == alpha_i:  \n",
    "            alpha_j = int(random.uniform(0, svm.numSamples))\n",
    "        error_j = calcError(svm, alpha_j)  \n",
    "      \n",
    "    return alpha_j, error_j  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the inner loop for optimizing alpha i and alpha j  \n",
    "def innerLoop(svm, alpha_i):\n",
    "    error_i = calcError(svm, alpha_i)  \n",
    "  \n",
    "    ### check and pick up the alpha who violates the KKT condition  \n",
    "    ## satisfy KKT condition  \n",
    "    # 1) yi*f(i) >= 1 and alpha == 0 (outside the boundary)  \n",
    "    # 2) yi*f(i) == 1 and 0<alpha< C (on the boundary)  \n",
    "    # 3) yi*f(i) <= 1 and alpha == C (between the boundary)  \n",
    "    ## violate KKT condition  \n",
    "    # because y[i]*E_i = y[i]*(f(i) - y[i]) = y[i]*f(i) - y[i]^2 = y[i]*f(i) - 1, so  \n",
    "    # 1) if y[i]*E_i < 0, so yi*f(i) < 1, if alpha < C, violate!(alpha = C will be correct)   \n",
    "    # 2) if y[i]*E_i > 0, so yi*f(i) > 1, if alpha > 0, violate!(alpha = 0 will be correct)  \n",
    "    # 3) if y[i]*E_i = 0, so yi*f(i) = 1, it is on the boundary, needless optimized  \n",
    "    if (svm.train_y[alpha_i] * error_i < -svm.toler) and (svm.alphas[alpha_i] < svm.C) or (svm.train_y[alpha_i] * error_i > svm.toler) and (svm.alphas[alpha_i] > 0):\n",
    "        # step 1: select alpha j  \n",
    "        alpha_j, error_j = selectAlpha_j(svm, alpha_i, error_i)  \n",
    "        alpha_i_old = svm.alphas[alpha_i].copy()  \n",
    "        alpha_j_old = svm.alphas[alpha_j].copy()\n",
    "  \n",
    "        # step 2: calculate the boundary L and H for alpha j  \n",
    "        if svm.train_y[alpha_i] != svm.train_y[alpha_j]:  \n",
    "            L = max(0, svm.alphas[alpha_j] - svm.alphas[alpha_i])  \n",
    "            H = min(svm.C, svm.C + svm.alphas[alpha_j] - svm.alphas[alpha_i])  \n",
    "        else:  \n",
    "            L = max(0, svm.alphas[alpha_j] + svm.alphas[alpha_i] - svm.C)  \n",
    "            H = min(svm.C, svm.alphas[alpha_j] + svm.alphas[alpha_i])  \n",
    "        if L == H:  \n",
    "            return 0  \n",
    "  \n",
    "        # step 3: calculate eta (the similarity of sample i and j)  \n",
    "        eta = 2.0 * svm.kernelMat[alpha_i, alpha_j] - svm.kernelMat[alpha_i, alpha_i] - svm.kernelMat[alpha_j, alpha_j] \n",
    "        if eta >= 0:  \n",
    "            return 0  \n",
    "  \n",
    "        # step 4: update alpha j  \n",
    "        svm.alphas[alpha_j] -= svm.train_y[alpha_j] * (error_i - error_j) / eta  \n",
    "  \n",
    "        # step 5: clip alpha j\n",
    "        if svm.alphas[alpha_j] > H:  \n",
    "            svm.alphas[alpha_j] = H  \n",
    "        if svm.alphas[alpha_j] < L:  \n",
    "            svm.alphas[alpha_j] = L  \n",
    "  \n",
    "        # step 6: if alpha j not moving enough, just return       \n",
    "        if abs(alpha_j_old - svm.alphas[alpha_j]) < 0.00001:  \n",
    "            updateError(svm, alpha_j)  \n",
    "            return 0  \n",
    "  \n",
    "        # step 7: update alpha i after optimizing aipha j  \n",
    "        svm.alphas[alpha_i] += svm.train_y[alpha_i] * svm.train_y[alpha_j] * (alpha_j_old - svm.alphas[alpha_j])  \n",
    "  \n",
    "        # step 8: update threshold b  \n",
    "        b1 = svm.b - error_i - svm.train_y[alpha_i] * (svm.alphas[alpha_i] - alpha_i_old) * svm.kernelMat[alpha_i, alpha_i] - svm.train_y[alpha_j] * (svm.alphas[alpha_j] - alpha_j_old) * svm.kernelMat[alpha_i, alpha_j]  \n",
    "        b2 = svm.b - error_j - svm.train_y[alpha_i] * (svm.alphas[alpha_i] - alpha_i_old) * svm.kernelMat[alpha_i, alpha_j] - svm.train_y[alpha_j] * (svm.alphas[alpha_j] - alpha_j_old) * svm.kernelMat[alpha_j, alpha_j]  \n",
    "        if (0 < svm.alphas[alpha_i]) and (svm.alphas[alpha_i] < svm.C):  \n",
    "            svm.b = b1  \n",
    "        elif (0 < svm.alphas[alpha_j]) and (svm.alphas[alpha_j] < svm.C):  \n",
    "            svm.b = b2  \n",
    "        else:  \n",
    "            svm.b = (b1 + b2) / 2.0  \n",
    "  \n",
    "        # step 9: update error cache for alpha i, j after optimize alpha i, j and b  \n",
    "        updateError(svm, alpha_j)  \n",
    "        updateError(svm, alpha_i)  \n",
    "  \n",
    "        return 1  \n",
    "    else:  \n",
    "        return 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main training procedure  \n",
    "def trainSVM(train_x, train_y, C, toler, maxIter, kernelOption = ('rbf', 1.0)):  \n",
    "    # calculate training time  \n",
    "    startTime = time.time()  \n",
    "  \n",
    "    # init data struct for svm  \n",
    "    svm = SVMStruct(mat(train_x), mat(train_y), C, toler, kernelOption)\n",
    "      \n",
    "    # start training  \n",
    "    entireSet = True  \n",
    "    alphaPairsChanged = 0  \n",
    "    iterCount = 0  \n",
    "    # Iteration termination condition:  \n",
    "    #   Condition 1: reach max iteration  \n",
    "    #   Condition 2: no alpha changed after going through all samples,  \n",
    "    #                in other words, all alpha (samples) fit KKT condition  \n",
    "    while (iterCount < maxIter) and ((alphaPairsChanged > 0) or entireSet):  \n",
    "        alphaPairsChanged = 0\n",
    "  \n",
    "        # update alphas over all training examples  \n",
    "        if entireSet:  \n",
    "            for i in range(0,svm.numSamples):  \n",
    "                alphaPairsChanged += innerLoop(svm, i)  \n",
    "            print('---iter:%d entire set, alpha pairs changed:%d' % (iterCount, alphaPairsChanged))  \n",
    "            iterCount += 1  \n",
    "        # update alphas over examples where alpha is not 0 & not C (not on boundary)  \n",
    "        else:  \n",
    "            nonBoundAlphasList = nonzero((svm.alphas.A > 0) * (svm.alphas.A < svm.C))[0]  \n",
    "            for i in nonBoundAlphasList:  \n",
    "                alphaPairsChanged += innerLoop(svm, i)  \n",
    "            print('---iter:%d non boundary, alpha pairs changed:%d' % (iterCount, alphaPairsChanged))  \n",
    "            iterCount += 1  \n",
    "  \n",
    "        # alternate loop over all examples and non-boundary examples  \n",
    "        if entireSet:  \n",
    "            entireSet = False  \n",
    "        elif alphaPairsChanged == 0:  \n",
    "            entireSet = True  \n",
    "  \n",
    "    print('Congratulations, training complete! Took %fs!' % (time.time() - startTime))  \n",
    "    return svm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing your trained svm model given test set  \n",
    "def testSVM(svm, test_x, test_y):  \n",
    "    test_x = mat(test_x)  \n",
    "    test_y = mat(test_y)  \n",
    "    numTestSamples = test_x.shape[0]\n",
    "    supportVectorsIndex = nonzero(svm.alphas.A > 0)[0]\n",
    "    supportVectors      = svm.train_x[supportVectorsIndex]  \n",
    "    supportVectorLabels = svm.train_y[supportVectorsIndex]  \n",
    "    supportVectorAlphas = svm.alphas[supportVectorsIndex]  \n",
    "    matchCount = 0\n",
    "    returnList=[]\n",
    "    for i in range(0,numTestSamples):\n",
    "        kernelValue = calcKernelValue(supportVectors, test_x[i, :], svm.kernelOpt)  \n",
    "        predict = kernelValue.T * multiply(supportVectorLabels, supportVectorAlphas) + svm.b\n",
    "        if sign(predict) == sign(test_y[i]):  \n",
    "            matchCount += 1\n",
    "        # print(test_y[i,0], sign(predict[0,0]))\n",
    "        returnList.append([test_y[i,0], sign(predict[0,0])])  # 保存结果,因为predict输出的为矩阵\n",
    "        # returnList.append([test_y[i].getA1(), sign(predict).getA1()])\n",
    "    accuracy = float(matchCount) / numTestSamples  \n",
    "    return returnList, accuracy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show your trained svm model only available with 2-D data  \n",
    "def showSVM(svm):  \n",
    "    if svm.train_x.shape[1] != 2:  \n",
    "        print(\"Sorry! I can not draw because the dimension of your data is not 2!\")\n",
    "        return 1  \n",
    "  \n",
    "    # draw all samples  \n",
    "    for i in range(0,svm.numSamples):  \n",
    "        if svm.train_y[i] == -1:  \n",
    "            plt.plot(svm.train_x[i, 0], svm.train_x[i, 1], 'or')  \n",
    "        elif svm.train_y[i] == 1:  \n",
    "            plt.plot(svm.train_x[i, 0], svm.train_x[i, 1], 'ob')  \n",
    "  \n",
    "    # mark support vectors  \n",
    "    supportVectorsIndex = nonzero(svm.alphas.A > 0)[0]  \n",
    "    for i in supportVectorsIndex:  \n",
    "        plt.plot(svm.train_x[i, 0], svm.train_x[i, 1], 'oy')  \n",
    "      \n",
    "    # draw the classify line  \n",
    "    w = zeros((2, 1))  \n",
    "    for i in supportVectorsIndex:  \n",
    "        w += multiply(svm.alphas[i] * svm.train_y[i], svm.train_x[i, :].T)   \n",
    "    min_x = min(svm.train_x[:, 0])[0, 0]  \n",
    "    max_x = max(svm.train_x[:, 0])[0, 0]  \n",
    "    y_min_x = float(-svm.b - w[0] * min_x) / w[1]  \n",
    "    y_max_x = float(-svm.b - w[0] * max_x) / w[1]  \n",
    "    plt.plot([min_x, max_x], [y_min_x, y_max_x], '-g')  \n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the two classifier:\n",
      "step 1: load data...\n",
      "step 2: training...\n",
      "---iter:0 entire set, alpha pairs changed:0\n",
      "Congratulations, training complete! Took 3.540578s!\n",
      "step 3: testing...\n",
      "step 4: show the result...\n",
      "The classify accuracy is: 54.487%\n",
      "[[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [1.0, 0.0], [0.0, 0.0], [1.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 0.0], [0.0, 0.0]]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nthe two classifier:')\n",
    "## step 1: load data  \n",
    "print(\"step 1: load data...\")  \n",
    "dataSet = []  \n",
    "train_size=0.7\n",
    "\n",
    "#path = 'iris.xlsx' \n",
    "#dataSet = np.loadtxt(path)\n",
    "#dataSet = np.array(dataSet)\n",
    "dfname='data/diabetes_data_upload'\n",
    "dataSet=pd.read_csv('%s.csv'%dfname,header=None)\n",
    "dataSet.iloc[1:,0]=dataSet.iloc[1:,0]\n",
    "dataSet=dataSet.values[1:,0:].astype('float')\n",
    "dataSet = np.array(dataSet)\n",
    "\n",
    "n0 = int(np.size(dataSet)/np.size(dataSet[0]))      # 总数据行数\n",
    "n1 = int(n0*train_size)\n",
    "x, y=dataSet.shape\n",
    "train_x = np.mat(dataSet[0:n1, 0:y-1]) \n",
    "train_y = np.mat(dataSet[0:n1, y-1]).T \n",
    "test_x = np.mat(dataSet[n1:n0, 0:y-1])\n",
    "test_y = np.mat(dataSet[n1:n0, y-1]).T \n",
    "\n",
    "## step 2: training...  \n",
    "print(\"step 2: training...\")  \n",
    "C = 10                                # 松弛变量\n",
    "toler = 0.001                        # 迭代终止条件\n",
    "maxIter = 10                        # 最大迭代次数\n",
    "kernelOption = ('rbf', 10)\n",
    "svmClassifier = trainSVM(train_x, train_y, C, toler, maxIter, kernelOption)  \n",
    "  \n",
    "## step 3: testing  \n",
    "print(\"step 3: testing...\")  \n",
    "resultList,accuracy = testSVM(svmClassifier, test_x, test_y)  \n",
    "\n",
    "## step 4: show the result  \n",
    "print(\"step 4: show the result...\")    \n",
    "print('The classify accuracy is: %.3f%%' % (accuracy * 100))  \n",
    "print(resultList)\n",
    "# showSVM(svmClassifier)  \n",
    "\n",
    "print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
